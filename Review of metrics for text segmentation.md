# Review of metrics for text segmentation

_____
* marx
* 2022-08-26
* Idee voor paper/Ruben Chapter for TOIS of IPM of ...
____

### scope

* only external metrics (so against a ground truth)
* only non overlapping segments (not all elements need to be in a block though)
* we also provide a repo with data, all metrics and runs/experiments

### metrics to cover

1. $p_k$, windowdiff
2. metrics from passage retrieval tasks
2. Semeval and MUC proposals for NER
3. metrics for extractive summarization
3. Clustering metrics reviewed by Amigo
4. metrics from signal processing (change point detectiuon)
    5. boundary F1, Rand index, Hausdorf are mentioned in [@truo_sele10]
    6. Hausdorf is a weird measure as it has no bound, and it simply goes for the largest error. 
        7. That is, in a stream with 100 docs, and the last one is very large, and the preiction has all correct, but adds an extra border almost at the end, we get a veryt large arror (i.e., the number of pages in that last doc minus the number of pages in that last predicted doc).
5. Metrics from compouyter vision
    6. pixel based
    7. combinations (PQ)

### How to group them?

1. Metrics based on a confusion table (P,R, F)
    2. counted on the elements
        3.  Bcubed
        4. purity and impurity
        5. Hamming (Damerau)  distance
    3. counted on the blocks 
        4. PQ
        5. NER proposals
    6. counted on sliding windows
        7. [@scai:gett12] WinPR measure
8. Metrics based on sliding window
    9. Pk, windowdiff and a lot of refinements, see [@scai:gett12]
2. Metrics based on counting pairs
    3. These are very unsuitable for linear segmentation, because they go against intuition: a small mistake is penalized much more if that is made on a large segment, than when it is made on a small segment. 
4. Entropy based measures
    5. Meaning is difficult.
5. Edit distance like metrics 
    6.  counting the number of transformations needed to go from one sequence to another.

#### Pk en wndowdiff

* Pevzner paper is prima, en hun maat is handig en een goede reparatie, alleen ze laten dus Probleem 5 eigenlijk niet zijn (wat betekent het nou, en wat heeft een ontwikkelaar eraan), ze komen er alleen in de conclusie heel even en best vaag op terug. 
* De 5 problemen die ze noemen zijn ook goed om langs te lopen voor PQ. 
* Het 5de punt wordt beter aangepakt in het [@scai:gett12] paper. 

### What do we (want to) do?

*a lot of the stuff below we have alfready from the wobir ictir paper* 

1. Implement every metric
2. Synthetic data (with all possible partitions)
3. Real data
4. Number of (strong) baseline approaches
    5. non learned
    6. learned/trained
5. Amigo or [moff_seve13](https://link.springer.com/chapter/10.1007/978-3-642-45068-6_1) consumentenbond achtige kwaliteits tabel

### Points of quality

#### Subjective Quality criteria

1. **Meaningfulness:** Perhaps the most important attribute of a metric is its plausibility as a measurement tool, that is, whether the scores it generates correlate with the underlying behavior it is intended to represent. [moff_seve13](https://link.springer.com/chapter/10.1007/978-3-642-45068-6_1)
2. **scrutability**, whether the score generated by the metric can be readily explained. [moff_seve13](https://link.springer.com/chapter/10.1007/978-3-642-45068-6_1)
3. **Helpful in error analysis**, whether the metric scores can guide developers of algorithms    to improve their performance.

#### other ideas

1. Properties of a metric
2. yes/no constraints/properties like Amigo and Moffat 
3. From [moff_seve13](https://link.springer.com/chapter/10.1007/978-3-642-45068-6_1)
    3. compare metrics according to their ability to attain similar system orderings on shared retrieval tasks; 
    4. or based on their likelihood of generating statistically significant pairwise system comparisons; 
    5. or based on their fit to observed user behavior.

1. Look at the distributions of the scores 
    2. pdf/kde
    3. cdf
    4. boplot
    5. normal vs lognormal
    6. variance shows "onderscheidend vermogen"
7. Metric must be able to *discriminate* between systems
    8.  But at what level?
        9. per sample
        10. or over a complete testset
    11. What does a difference mean?
        12. directly related to the measurement level
    13. Significance tests.
14. Aggregates (like a mean, like MAP) versus distributions
    15. 2 systems can have the same MAP but score totally different on each sample.


## Additional research questions

1. How does the additional linaeir order on the domain influcnece the clustering metrics? *beter formuleren*
2. How do the metrics "react" on overlapping gold stanbdard, or overklapping predictions.
3. How can we deal with fuzzy/soft borders? 
    4. in signal processing hoeft het niet zo precies
    5. allicht kan je ok in andere toepassingen een (klein) foutje makkelijk repoareren laer, of vind een gebruiker dat niet erg)
    6. Bij image segmentatin is het heel lastig de ground truth consistent aan te geven. Bij NER ook (zie ons DATA papefr over dataset NER).


______

* This may be a whole research direction on its own.


## Overlapping segments or fuzzy boundaries

* we may think of systems which give a small band of change/boundary points, with the elements in the band (partially) ranked as being the most probable real changepoint.
* Then we have overlapping segments by definition.
* Not strange from an IR perspective
    * we can index the additional words to the document, possibly with a lower weight, of some extra way of loading those. (so we always take the largest segments as documents to index)
    * also, when the system has the ability to present the document, it can show the largest segment, but let it start at the most probable page, and possibly indicate with colors/heatmap like the probability that a page does not belong to the document.

### Needed then

* Segmentation/clustering systems which can provide such fuzzy borders.
* Adjustments of the metric(s), similar to the ranked based P and R scores in IR compared to the P and R defs in classification.   


## Constraints

**RQ** 

1. What are reasonable constraints for a text segmentation metric?
2. Does PQ satisfy the 4 constraints of Amigo?
    1. The first three only when they are weakened to $\leq$. The last one fails.
    2. This is due to the definition of TP with the $IoU(H,T)>.5$ constraint. The resulkts hold both for weighted and "unweighted" PQ (so when we do not multiplu by the mean IoU). 
## The four from Amigo 2008

###  Constraint 1: cluster homogeneity

PQ fails this constraint; but satisfies it as the relation is $\leq$. You can make a counterexample in 2D or in 1D, simpoly have a mixed cluster C which contains points from 2 real segments A and B, but IoU(C,A) and IoU(C,B) is not more than a half. And also $IoU(C\cap A, A)$ and $IoU(C\cap B, B)$ are not more than a half. 

So you cannot make it worse by splitting a non homogeneous cluster, but it might be that you also do not improve.

###  Constraint 2: cluster completeness

Same as for homogeneity: PQ onlhy satisfies it with $\leq$, as is easy to see: Have a real segment C, and two (small) predicted subsegments A and B from C, such that $IoU(A\cup B,C)$ is still not more than a half. Again it is easy to see that you cannot make it worse by merging two subsegments from the same real segment.

### Constraint 3: rag bag

Again, it only holds for $\leq$. If the clean cluster $C_{clean}\subseteq C_{big}$ is such that $IoU(C_{clean}, C_{big}) \not> .5$ then there will be no improvent in PQ score by removing that one bad element $b$  from $C_{clean} \cup \{b\}$. Again it cannot be made worse.

### Constraint 4: clusters size versus quantity

THis can in fact fail for PQ. Counterexample: (based on Figure 4 in Amigo). All the 2-size clusters are too small to be TPs, so no gain there, but the $n+1$ size cluster $C_1\subseteq C$ is large enough to be a TP ($IoU(C_1,C)>.5$), but the $n$-size $C_1'$ is just too small. E.g., let $|C| = 2n$.  


## Other overviews

* [@purv:topi11] section 1.4 describes boundary F1, pk windowdiff, and "content based" without givinbg a definition, I think he means P, R , F over segments. As he mentions accuracy.
    * He diovides the metrics into (in my terms) point based confusion matrix, segmentation based (windowdiff like), and segment based confusion matrix.
    * He also gives a "repair" of windowdiff on page 12: $Pr_{error}$
* [@scai:gett12] gives yet another repair of windowdiff and also mentions a few more others. 
    * Based on the same idea as windowdiff it creates notions of TP, FP and FN's and thus can compute windowP and windowR measures and then of course also an F measure. 
    * **It has 5 excellent easy examples which are also great for showing how PQ works**
        * The examples also show that you need to multiply by the SQ because if you do not then also non perfect predictions will get a score of 1. (in fact you also have this when you allow a boundary error margin, but alas).
