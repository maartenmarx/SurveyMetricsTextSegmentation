# A survey of metrics for text segmentation

# Introduction

## The task: text segmentation
The task we consider is linair segmentation, that is, given a sequence of elements  $S$ (a linearly ordered set), partition $S$ such that each block is a subsequence of $S$.
We often call the blocks *segments*.
As we demand a partition, segments do not overlap and together cover $S$ (each element in $S$ belongs to exactly one segment).

We only consider metrics for external evaluation, that is, given a sequence $S$, we compare a true segmentation $T$ of $S$ to a hypothetical or predicted segmentation $H$ of $S$.

This survey is restricted to segmentations of text, which is obviously linearly ordered, but, dependingv on the task at had, the elements can range from chracters via words and passages to any meaningful subsequence. Table~1 lists some well studied text segmentation tasks, indicating the elements and to what extent they can be seen as partition problems.


| **Instances of text segmentation** | **elements**|**no overlap**|**complete coverage**|
|:--|:--|:--:|:--:|
|word tokenization| characters| $\checkmark$| $\checkmark$|
|sentence tokenization problems | tokens| $\checkmark$| $\checkmark$|
|paragraph tokenization problems | sentences| $\checkmark$| $\checkmark$|
|topical segmentation| sentences| $\checkmark$| $\checkmark$|
|page stream segmentation| pages |$\checkmark$| $\checkmark$|
|Named entity recognition| tokens | $\checkmark$|  $!$|
|Extractive summarization| tokens | $\checkmark$|  $!$|

**Table 1: Examples of text segmentation problems, with the elements in the text sequences and to what extent they can be seen as partition problems.**

Text segmentation is a clustering task with an additional constraint. Some closely related tasks are *change point detection* over multivariate time series [@truo:sele19], object detection in 2D images [@ panoptic quality paper], and community detection in (social) networks [@fortunato survey].
In the majority of instances, methods and metrics work with partitions. Sometimes they do not cover all elements (e.g. DBSCAN, image recognition) and rarely  allow overlapping clusters (k-clique communities [@palla,nature]).
For external evaluation megtrics, non coverage is typically not a problem as the not coverd elements can be considered as the (not interesting) true negatives.
Overlapping clusters can lead to problems of double counting and normalization, making it hard to define well behaved metrics.
In this survey, we do not consider overklapping segments but mention, when appropriate, non covering segmentations.

## How we will  compare the metrics

Surveys of metrics tend to have a leaderboard like table with the surveyed metrics on the rows and each column containing a  quality indicator by which the metrics are compared. These indicators can be divided into two groups: those that can be formally verified like 

* **boundedness**, the metric takes values in a bounded interval,

and those that can only be argued for, like 

* **scrutiny**, the score generated by the metric can be readily explained [@moff:seve13].

Other surveys use the terms objective and subjective for this division, but as Alistair Moffat rightly points out, the choice of the objective indicators is itself subjective [@moff:seve13].
The metrics commonly used tend to score well on the proposed formally verifiable metric properties [loads of refs, @amig:comp08, @pevz:crit02, @moff:seve13, refs mentione dby amigo], so we will use them sparsely and concentrate on those that need an argument. 
In more or less their order of importance, these are *meaningfullness*, the already mentioned *scrutability*, *helpfulness*, and *distinctiveness* and *room to grow*. 

\begin{description}
\item[meaningfullness] its plausibility as a measurement tool, that is, whether the scores it generates correlate with the underlying behavior it is intended to represent [@moff:seve13]. 
\item[scrutiny] the score generated by the metric can be readily explained.
\item[helpfulness] how useful it is in error analysis and improving a  system.
\item[distinctiveness] the likelihood of statistically significant system differentiations being obtained.
\item[room to grow] For bounded metrics, eg in the interval $[0,1]$, we want that there is "room to grow" from the scores of unlearned baselines or an out of the box method like agglomarzative clustering on a simple representation of the  elements in the sequence. 
\end{description}

## The space of text segmentation metrics

Before we get formal, we group the discussed metrics.
There are two main groups, even with overlap and a few which are used in clsutering but not well applicable to segmentation.

### Group 1: metrics based on sliding window.

All these metrics use a window of size $k$ (a hyperparameter usually set to half the average segment size) that is slided over the true and hypothesized segmentations in sync and measures differences.
The final value is the mean over all possible window slidings.
The orginal $p_k$ from [@beef:stat99] has led to a chain of well argued improvements. $p_k$ is derived from a metric which captures   *the probability that two sentences drawn randomly from the corpus are correctly identified as belonging to the same document or to different documents*.
The most commonly used version is *WindowDiff* which yields for each window a Boolean value indicating whether the number of segments boundaries under the window is the same in the true and predicted segment [@pevz:crit02].
This chain, well described in [@scai:gett12] has ended in a proposal, WinPR, which also belongs to the second main group.

### Group 2: metrics based on a confusion table

A confusion table divides items in true and false positives and negatives and is used to calculate precision, recall and combined $F_{\beta}$ scores.
As we only have two partitions with the shard domain being  the only connection between them, it is not straightforward to classify the items, and not even which items should be classified.
We further divide this group based on the items in the confusion table. 

Closest to the task at hand is when the items are the segments. Examples are the various metrics proposed for NER (with various degrees of partial and strict matching) and  for object segmentation like panoptic quality [@].
Recall then simply counts which portion of the true segments is also predicted.

Other measures take the elements in the sequence as items in the confusion table: purity [@zhao:crit01], BCubed and ELM [@bier,@amig,@heus:ele22], are in essence classification metrics for the question whether an element is correctly classified as a boundary of a segment. TODO:ECHT???
The Hamming distance between the true and predicted segmentation represented as a bit vector indicating the boundaries equals accuracy and also belongs to this group.

Finally [@scai:gett12] proposed a metric WinPR which take each item in the segment together with its surrounding sliding windows of the previous group as items in the confusion table.

### Other groups

Metrics in the next groups have been mentioned but are not common, mainly because they do not fit the task very well. 
Metrics based on string transformations, like th ejust discussed Hamming distance do make sense, depending on the transformations being allowed.  The Damerau transformation of swapping with a neighbor makes sense, and can be used to repair small errors. The Levenshtein transformations of insertion and deletion are not suitable for our task as they can repair any mistake with at most 2 steps.

Metrics based on counting pairs like the Rand index are not suitable  because of the quadratic impact they punish mistakes in large segments much more than those in smaller ones.
For clustering, several entropy based metrics have been proposed, but as far as we know they have not been applied to text segmentation. Besides that [@amig:comp09] show that they do not satisfy some basic clustering desiderata.


##### range, good-bad, can they be 0?

* error metrics
* 

## Main findings

* Probably that PQ is the best


# All  the metrics (ruben)

## Technicalities

* all formal notation, really nicey uniform
* like in the ELM paper
* The task formal easy, domain, elements, segments
    * all "degenerate" baselines
    * becuase 1D (lineair): 
        *   binary predicting first element equivalent to  segmentation
        *   but also "predicting neighbours in same document" equivalent to segmentation
* The True and Hypothesized  segments
* All the metrics, nciocely grouped
    * confusion matrix
    * window based
    * if needed, quickly some of the others

## Theoretical results

* Logical properties of the metrics and relations between them
    * range, open or closed, thus min and max
    * high is good or bad?
    * Max score iff perfect match ???
    * Min score iff completey wrong
    * It would be great if we could already show some clear results on the goodness/generality of PQ (ie. that it repairs and generalizes NER partial match, and approaches NER perfect match )

 
# Evaluation of the metrics


## Empirical results: comparing the metrics   (ruben)


### TODO

* welke taken (PSS, NER?, NER met flink langere entities, misschien zelfs hierarchical NER, extractive summarization?, text segmentation?)
    * Moeten goede systemen en goede datasets voor zijn (dus net als bij het PQ paper)
    * geen moeilijk gedoe voor onszelf: gewoon installeren en runnen.
    * Liefst 3 systemen per taak, zodat we ook die "distinctiveness kunnen laten zien"
* Wat meten we, en wat tonen we, en wat is onze boodschap nou?

### eerdere eideeen 

* **Onze gave KDE plaatjes en overzichten op een taak**
* 
Before we compare the metrics on the quality criteria, let us first see them in action, using a real dataset and a serious segmentation method, and synthetic data and all possible segmentations. We scale all metrics into the real interval $[0,1]$, and we are mostly interested in the distributions of the scores over a seriously sized test-set. We will also look at correlations between the metrics and how well we can fit a lineair transformation between them. 

* experiment 1: a strong ssegmentation system on a mix of data and then all kde's, cdfs, and correlation matrix, and regression models between the metrics.
    * **does that makes sense? Giuven that you know the value of a metric on an instance can you preduct the value of another metric on that same instance?** 
* Test the regression models on another testset, and mossibly aslo another method.
* experiment 2: like in the bcubed-elm paper, again show the kde's and cff of all metrics but now over a fixed testset, and all possible segmentations. 
* We can also do the regresiion fitting and testing heree. 


## Evaluating the metrics one by one


### Introduction

> waarop evalueren we? hoe pakken we het aan?

* In each section we shoulld end with a clear overview of all othe metrics from that section on our  4/5 quality criteria
*  ability to discriminate
    * based on the variance, shape
* qualitative evaluation
    * tabel met de 4 indicatoren
    * verhaaltje per metric
        * window diff, en WinPR
        * bcubed, elm
        * purity
        * boundary P, R, F1
        * NER perfect match, partial match
        * PQ, 
        * Systems based on some "optimal mapping" (purity does this in a sense, also some of the Amigo measures have that)
    


### Confusion table based


#### Metrics based on aligning segments

* PQ, optimal alingnment
* mathematical properties    

### Moving window based

* Pk stuk, met mooie conclusie



#### $P_k$ and the degenerate systems claim

The metric $P_k$ from [@beef_stat99]() is a simpler version from the metric $PD$ by the same authors [refneeded]. $PD$ is is the probability that two sentences drawn randomly from the corpus are correctly identified (as belonging to the same document or to different documents). The much easier to implement version $P_k$ is described by  Pevzner and Hearst [@pevz_grit02]() as follows: "$P_k$ is calculated by setting $k$ to half of the average true segment size and then computing penalties via a moving window of length $k$. At each location, the algorithm determines whether the two ends of the probe are in the same or different segments in the reference segmentation and increases a counter if the algorithm's segmentation disagrees. The resulting count is scaled between 0 and 1 by dividing by the number of measurements taken". 

$PD$ has a clear intuitive and mathematically sound meaning, but is hard to compute. The easy to implement  $P_k$ is also easily understood, but it is not immediate that it is a natural metric for sementation problems. Indeed the 5th problem Pevzner and Hearst indicate for $P_k$ ---and the only one that is not solved by their *WindowDiff* repair--- is that it is not clear what the scores mean [@pevz_grit02]. 
But [@beef_stat99] contains another justification for $P_k$: 

> It turns out empirically, and can be shown analytically (under strong assumptions), that if the window k is chosen to be half the average reference segment length (in words), then all of the major “degenerate” algorithms—hypothesizing boundaries everywhere, uniformly, randomly, and not at all—have nearly the same low score of Pk ≡ PDk ≈ 1/2 (Figure 12). With this justification, we use the error metric Pk
in our quantitative analysis.


We will now  test whether the emperical part of this claim holds on a large PSS dataset. We test it not just for $P_k$ but also for the later suggested improvements. We answer the following questions:
 
1.   Does the claim about the degenerate segmenters all scoring roughly  $P_k=.5$ on real world datasets also hold for page segmentation datasets, when the items are much coarser (i.e., pages instead of words)?
2.    Does the same hold for suggested improvements of $P_k$, in particular WindowDiff and WinPR?
3.   What is the influence of the value of $k$ on the claim?
4.   What is the influence of the padding improvement suggested by [Lamprier (2008)].
<!-- 5. <strike>What is the influence of not counting the first page of a stream as a boundary? This sounds reasonable as the first page is always a boundary, so why to give an segmenter credit for having that correct.</strike>  * Hardly any influence, except on all boundaries, whose error goes up.   --> 

We answer the questions using the WooIR $D1$ dataset  from [@heus_wooi23] which contains 333 streams with in total 31.293 pages and 134.460 pages. We consider 5 "degenerate" non-learned baselines: every page is a cluster (all), the whole stream is one cluster (none), two uniformly spaced clusterings, based on the mean and median document length in the stream, respectively (the last cluster may be shorter), and a random baseline.
The random prediction is created as follows: given the stream length, first randomly determine the number of documents in the stream, and then, making sure the stream is filled correctly, randomly assign a  length to each document. All these "degenerate" systems are in fact rule based baselines whose rules rely only on priors measured at the level of the corpus. 

The effect of the padding suggested by Lamprier et.al [@lamp_eval08] was absent for all 5 baselines except random, for which the difference in mean scores was at most $.008$ (for mean precision). Because the  padding makes intuitive sense, we do all experiments with this so-called Lamprier padding.

For the first question, Tabel 1 contains the results for $P_k$. Figure 1 shows the corresponding population density plots for the 333 streams. On this dataset, the claim only holds for the random system and the system which makes every page a seperate cluster. The mean system is already a strong baseline with  $P_k=.39$. Recall that $P_k$ is an error score, so lower is better. 


**Table 1: Mean $P_k$ per stream (N=333), for the 5 degenerate segmentation methods.**

|        |   mean |   std |
|:-------|-------:|------:|
| all    |   0.49 |  0.15 |
| none   |   0.42 |  0.2  |
| median |   0.42 |  0.21 |
| mean   |   0.39 |  0.17 |
| random |   0.49 |  0.16 |

**Figure 1: Population density plots for  $P_k$ per stream (N=333), for the 5 degenerate segmentation methods.**
![](Pk_experiment.png)

The answer to the second question is given in Table 2 and Figure 2. Note that WinPR is an F1 measure, so higher is better, and WindowDiff is an error measure as $P_k$. For both measures, the means are spread  much more over the five degenerates than for $P_k$, and are even further removed from the desired "middle" error value. So, we can conclude that these two metrics repair certain shortcomings of $P_k$, but in doing so seem to destroy the justification for using $P_k$.


**Table 2: Mean $P_k$, WinPR and WindowDiff per stream (N=333), for the 5 degenerate segmentation methods.**


|        |   Pk |   WinPR |   WindowDiff |
|:-------|-----:|--------:|-------------:|
| all    | 0.49 |    0.4  |         0.88 |
| none   | 0.42 |    0.22 |         0.43 |
| median | 0.42 |    0.62 |         0.52 |
| mean   | 0.39 |    0.59 |         0.45 |
| random | 0.49 |    0.44 |         0.67 |

**Figure 2: Population density plots for  WinPR and WindowDiff per stream (N=333), for the 5 degenerate segmentation methods.**
![](WindowDiffAndWinPR_experiment.png)

To end,  we look at the influence of the definition of the hyperparameter $k$, the size of the window, on the claim. This parameter is suggested to be set at half the mean cluster size of a sample. So in our case, half of the mean document size in a stream. 
Figure 3 shows the effect of widening the window, and we see that for all degenerates (except no cluster), the mean $P_k$ goes down when we use a wider window. Several papers mention that $k$ is a tuneable hyperparameter. But this experiment shows that indeed with larger windows, the error score goes down, but this seems to be an artefact. In any case, increasing the window size further weakens the justification for $P_k$. 

**Figure 3: the effect of increasing the window size on the mean $P_k$ (N=333).**
![](Kexperiment.png)


### rest group

* Kort, vooral literatuur aanhalen over slechte eigenschappen


# Hierarchical, Fuzzy border  and Overlapping segmentation

* Bespreek hierarchical NER, en hoe zij het probleem oplossen. Nested NER Finkel Manning paper
* Bespreek Amigo CICE-BCUbed aanpak. kort want oninterresant
* Fuzzy borders: PSS, image PQ, boundary PQ
* Overlapping: 3 soorten
    * k-clique achtig, duidelijk niet overlappende core, grens is echt een grens
    * amigo: alles mag
    * clustered clustering: set of partial partitions.
        * zonder labels per set
        * Dus je moet nu 2 keer een match gaan vinden
            * op set niveau
            * op block niveau
        * Dat kan heel goed met PQ, en dan de set-matching nemen met de hoogtse som van PQs. Dat is wel duur, maar die sets zijn toch niet zo groot? En is dit niet consraint satisfaction? Of knap-sack? 

# Conclusion

* Following  the evaluation of the NER metrics, we come to a nice balanced version between partial and exact match: PQ is best 
